{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CliffWalkingEnv.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SankalpA11/DRL/blob/OPEN-AI-gym/CliffWalkingEnv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUI2yQGYEguo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from gym import Env, spaces\n",
        "from gym.utils import seeding\n",
        "\n",
        "def categorical_sample(prob_n, np_random):\n",
        "    \"\"\"\n",
        "    Sample from categorical distribution\n",
        "    Each row specifies class probabilities\n",
        "    \"\"\"\n",
        "    prob_n = np.asarray(prob_n)\n",
        "    csprob_n = np.cumsum(prob_n)\n",
        "    return (csprob_n > np_random.rand()).argmax()\n",
        "\n",
        "\n",
        "class DiscreteEnv(Env):\n",
        "\n",
        "    \"\"\"\n",
        "    Has the following members\n",
        "    - nS: number of states\n",
        "    - nA: number of actions\n",
        "    - P: transitions (*)\n",
        "    - isd: initial state distribution (**)\n",
        "    (*) dictionary dict of dicts of lists, where\n",
        "      P[s][a] == [(probability, nextstate, reward, done), ...]\n",
        "    (**) list or array of length nS\n",
        "    \"\"\"\n",
        "    def __init__(self, nS, nA, P, isd):\n",
        "        self.P = P\n",
        "        self.isd = isd\n",
        "        self.lastaction = None # for rendering\n",
        "        self.nS = nS\n",
        "        self.nA = nA\n",
        "\n",
        "        self.action_space = spaces.Discrete(self.nA)\n",
        "        self.observation_space = spaces.Discrete(self.nS)\n",
        "\n",
        "        self.seed()\n",
        "        self.s = categorical_sample(self.isd, self.np_random)\n",
        "        self.lastaction=None\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def reset(self):\n",
        "        self.s = categorical_sample(self.isd, self.np_random)\n",
        "        self.lastaction = None\n",
        "        return self.s\n",
        "\n",
        "    def step(self, a):\n",
        "        transitions = self.P[self.s][a]\n",
        "        i = categorical_sample([t[0] for t in transitions], self.np_random)\n",
        "        p, s, r, d= transitions[i]\n",
        "        self.s = s\n",
        "        self.lastaction = a\n",
        "        return (s, r, d, {\"prob\" : p})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HDn5SZNEZlW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "from gym.envs.toy_text import discrete\n",
        "\n",
        "UP = 0\n",
        "RIGHT = 1\n",
        "DOWN = 2\n",
        "LEFT = 3\n",
        "\n",
        "\n",
        "class CliffWalkingEnv(discrete.DiscreteEnv):\n",
        "    \"\"\"\n",
        "    This is a simple implementation of the Gridworld Cliff\n",
        "    reinforcement learning task.\n",
        "    Adapted from Example 6.6 (page 106) from Reinforcement Learning: An Introduction\n",
        "    by Sutton and Barto:\n",
        "    http://incompleteideas.net/book/bookdraft2018jan1.pdf\n",
        "    With inspiration from:\n",
        "    https://github.com/dennybritz/reinforcement-learning/blob/master/lib/envs/cliff_walking.py\n",
        "    The board is a 4x12 matrix, with (using Numpy matrix indexing):\n",
        "        [3, 0] as the start at bottom-left\n",
        "        [3, 11] as the goal at bottom-right\n",
        "        [3, 1..10] as the cliff at bottom-center\n",
        "    Each time step incurs -1 reward, and stepping into the cliff incurs -100 reward\n",
        "    and a reset to the start. An episode terminates when the agent reaches the goal.\n",
        "    \"\"\"\n",
        "    metadata = {'render.modes': ['human', 'ansi']}\n",
        "\n",
        "    def __init__(self):\n",
        "        self.shape = (4, 12)\n",
        "        self.start_state_index = np.ravel_multi_index((3, 0), self.shape)\n",
        "\n",
        "        nS = np.prod(self.shape)\n",
        "        nA = 4\n",
        "\n",
        "        # Cliff Location\n",
        "        self._cliff = np.zeros(self.shape, dtype=np.bool)\n",
        "        self._cliff[3, 1:-1] = True\n",
        "\n",
        "        # Calculate transition probabilities and rewards\n",
        "        P = {}\n",
        "        for s in range(nS):\n",
        "            position = np.unravel_index(s, self.shape)\n",
        "            P[s] = {a: [] for a in range(nA)}\n",
        "            P[s][UP] = self._calculate_transition_prob(position, [-1, 0])\n",
        "            P[s][RIGHT] = self._calculate_transition_prob(position, [0, 1])\n",
        "            P[s][DOWN] = self._calculate_transition_prob(position, [1, 0])\n",
        "            P[s][LEFT] = self._calculate_transition_prob(position, [0, -1])\n",
        "\n",
        "        # Calculate initial state distribution\n",
        "        # We always start in state (3, 0)\n",
        "        isd = np.zeros(nS)\n",
        "        isd[self.start_state_index] = 1.0\n",
        "\n",
        "        super(CliffWalkingEnv, self).__init__(nS, nA, P, isd)\n",
        "\n",
        "    def _limit_coordinates(self, coord):\n",
        "        \"\"\"\n",
        "        Prevent the agent from falling out of the grid world\n",
        "        :param coord:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        coord[0] = min(coord[0], self.shape[0] - 1)\n",
        "        coord[0] = max(coord[0], 0)\n",
        "        coord[1] = min(coord[1], self.shape[1] - 1)\n",
        "        coord[1] = max(coord[1], 0)\n",
        "        return coord\n",
        "\n",
        "    def _calculate_transition_prob(self, current, delta):\n",
        "        \"\"\"\n",
        "        Determine the outcome for an action. Transition Prob is always 1.0.\n",
        "        :param current: Current position on the grid as (row, col)\n",
        "        :param delta: Change in position for transition\n",
        "        :return: (1.0, new_state, reward, done)\n",
        "        \"\"\"\n",
        "        new_position = np.array(current) + np.array(delta)\n",
        "        new_position = self._limit_coordinates(new_position).astype(int)\n",
        "        new_state = np.ravel_multi_index(tuple(new_position), self.shape)\n",
        "        if self._cliff[tuple(new_position)]:\n",
        "            return [(1.0, self.start_state_index, -100, False)]\n",
        "\n",
        "        terminal_state = (self.shape[0] - 1, self.shape[1] - 1)\n",
        "        is_done = tuple(new_position) == terminal_state\n",
        "        return [(1.0, new_state, -1, is_done)]\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        outfile = sys.stdout\n",
        "\n",
        "        for s in range(self.nS):\n",
        "            position = np.unravel_index(s, self.shape)\n",
        "            if self.s == s:\n",
        "                output = \" x \"\n",
        "            # Print terminal state\n",
        "            elif position == (3, 11):\n",
        "                output = \" T \"\n",
        "            elif self._cliff[position]:\n",
        "                output = \" C \"\n",
        "            else:\n",
        "                output = \" o \"\n",
        "\n",
        "            if position[1] == 0:\n",
        "                output = output.lstrip()\n",
        "            if position[1] == self.shape[1] - 1:\n",
        "                output = output.rstrip()\n",
        "                output += '\\n'\n",
        "\n",
        "            outfile.write(output)\n",
        "        outfile.write('\\n')\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}